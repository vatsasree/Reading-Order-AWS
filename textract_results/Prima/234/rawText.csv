'Page Number,'Type,'Text,'Confidence Score % (Line),
"'1","'LINE","'408","'99.95632172"
"'1","'LINE","'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 4, APRIL 2001","'96.62071228"
"'1","'LINE","'variables Pkj ,Sj). Thus, T represents a generalization","'86.06921387"
"'1","'LINE","'3.4 Computational Considerations","'99.27474976"
"'1","'LINE","'based on subsample order statistics [35] of the Wilcoxon-","'99.69952393"
"'1","'LINE","'For large sample sizes such as those encountered in the","'99.92098236"
"'1","'LINE","'Mann-Whitney statistic. The statistic T is a U-statistic and","'99.34645081"
"'1","'LINE","'olfactory classification task, the calculation of the observed","'98.87443542"
"'1","'LINE","'can be shown to have an asymptotically normal distribution","'99.56722260"
"'1","'LINE","'value of T via (4) and of the distribution FT via the available","'96.14146423"
"'1","'LINE","'[35]. However, for small sample sizes-and especially for","'99.49843597"
"'1","'LINE","'recurrence, are computationally intensive exercises. For the","'99.40998077"
"'1","'LINE","'unbalanced designs or unequal subset sizes Kj, in which","'96.29711151"
"'1","'LINE","'example, results presented in Sections 4.2 and 4.3, the","'99.80610657"
"'1","'LINE","'case FT is skewed-inference based on the asymptotic","'97.36807251"
"'1","'LINE","'following estimators are used.","'99.79886627"
"'1","'LINE","'distribution is inappropriate; the exact distribution (or a","'99.56439972"
"'1","'LINE","'Let Su be a uniform random sample of size U from the","'98.73169708"
"'1","'LINE","'small-sample approximation thereof-see Section 3.4) is","'99.82421112"
"'1","'LINE","'collection of subset pairs A. The estimator for T is given by","'96.57561493"
"'1","'LINE","'necessary. A recurrence for the exact distribution FT under","'96.73671722"
"'1","'LINE","'the null hypothesis H is given in [25] for ko k1 = 1.","'91.98957062"
"'1","'LINE","'(9)","'99.88701630"
"'1","'LINE","'Classification based on the generalized Wilcoxon-Mann-","'99.62396240"
"'1","'LINE","'(So,S1)ESu","'14.74119377"
"'1","'LINE","'Whitney statistic T given in (4) is particularly relevant to","'99.77909851"
"'1","'LINE","'The estimator standard deviation OF < 1/(2Vu), indicating","'80.62362671"
"'1","'LINE","'interpoint distance-based nonparametric discriminant ana-","'98.50508118"
"'1","'LINE","'how large U must be taken (and, consequently, the required","'98.99826050"
"'1","'LINE","'lysis [25]. Different choices for the parameters yield","'99.78946686"
"'1","'LINE","'computational demand) in order to have an estimator with","'99.86613464"
"'1","'LINE","'desirable power characteristics against different alternatives","'99.65461731"
"'1","'LINE","'some prescribed accuracy. Equation (9) can be employed","'99.87276459"
"'1","'LINE","'[35]. The issue of adaptively selecting the parameters","'99.66636658"
"'1","'LINE","'using either observed data or sequences generated under","'99.84078217"
"'1","'LINE","'ro,r1,ko,k1 will be addressed in Section 3.6.","'92.70870209"
"'1","'LINE","'the null hypothesis. To obtain the quantile estimator for FT,","'96.29685974"
"'1","'LINE","'3.2 Extension to K > Classes","'96.49642181"
"'1","'LINE","'we consider a collection {T1, 7v} of such estimators taken","'86.06327057"
"'1","'LINE","'under H. Then,","'82.19278717"
"'1","'LINE","'The proposed classifier has been developed for the simple","'99.82878113"
"'1","'LINE","'two-class problem. The extension to K > 2 classes can be","'97.64963531"
"'1","'LINE","'addressed in two ways. The statistic T can be generalized to","'99.84194183"
"'1","'LINE","'(10)","'99.78125763"
"'1","'LINE","'the K sample case and a recurrence for the joint distribution","'99.78902435"
"'1","'LINE","'FT1,...,TK is available [25]. Another approach is that of","'91.24135590"
"'1","'LINE","'with an accuracy dependent on U, V, and t.","'95.07530212"
"'1","'LINE","'addressing the K class problem through consideration of","'99.75363922"
"'1","'LINE","'3.5 Classifier Consistency","'99.86431122"
"'1","'LINE","'a collection of two class subproblems [9], [14].","'99.54537201"
"'1","'LINE","'Our hypothesis testing approach to the two-class decision","'99.81060791"
"'1","'LINE","'3.3 Relationship to Machine Learning","'99.86126709"
"'1","'LINE","'problem ([1], p. 183) allows us to address the issue of","'99.13213348"
"'1","'LINE","'From a machine learning perspective, the classifier g based","'99.70967102"
"'1","'LINE","'""classifier consistency"" from the standpoint of consistent","'99.36483002"
"'1","'LINE","'on T is a classic example of ""classification by ensemble""","'99.12927246"
"'1","'LINE","'tests of hypotheses. Note that the null hypothesis H F =","'94.55764008"
"'1","'LINE","'[11], [8], [20]. The statistic T represents the most funda-","'99.44437408"
"'1","'LINE","'F1 implies Fp(z,X;|Y;=0) = Fp(z,X;|Y;=1) for any fixed observation","'87.96670532"
"'1","'LINE","'mental approach to constructing ensembles of classifiers. In","'99.64374542"
"'1","'LINE","'Z. For simplicity, consider as alternative hypotheses stochastic","'95.84655762"
"'1","'LINE","'a manner similar to bagging [2], subsamples So and S1 are","'91.89744568"
"'1","'LINE","'ordering; the random variable p(z, Xi|Yi = 0) is defined to be","'91.71870422"
"'1","'LINE","'taken (without replacement) from the training database Dn,","'95.44141388"
"'1","'LINE","'stochastically smaller than XiY 1), denoted as","'86.47893524"
"'1","'LINE","'and represents a classifier for Z based on","'99.57350159"
"'1","'LINE","'these subsamples. Thus, all possible subsample classifiers","'99.70262146"
"'1","'LINE","'obtained are then combined in T via the simplest possible","'99.78171539"
"'1","'LINE","'if","'99.94911194"
"'1","'LINE","'method for combining individual classification decisions","'99.81611633"
"'1","'LINE","'from an ensemble of classifiers: an unweighted vote.","'99.63541412"
"'1","'LINE","'Observing a value of T 1/2 means that a majority of the","'98.56822205"
"'1","'LINE","'for every X, with strict inequality for at least one X. From","'95.02910614"
"'1","'LINE","'subclassifiers in the ensemble favors","'99.48683167"
"'1","'LINE","'Fig. 3, we see that, for the left panel in which the test","'99.87075806"
"'1","'LINE","'class 1. A more appropriate classification criterion is the","'99.80358124"
"'1","'LINE","'observation (TCIf7712) is TCE-present (class 1), we have","'94.49338531"
"'1","'LINE","'event {I{F(7(2)>1/2)} 1}. This event represents evidence in","'89.56080627"
"'1","'LINE","'p(z,Xi|Y = 0) st = For the TCE-absent ob-","'85.64478302"
"'1","'LINE","'favor of class 1 VS. class 0 in that the vote count is","'97.56379700"
"'1","'LINE","'servation (Kero0203) depicted in the right panel of Fig. 3,","'99.51495361"
"'1","'LINE","'probabilistically large (under H). Thus, the classifier proposed","'93.29020691"
"'1","'LINE","'p(z,Xi|Y = as desired for a class 0","'87.51819611"
"'1","'LINE","'in (6), based on the unweighted ensemble vote T, accounts for","'99.34371948"
"'1","'LINE","'observation. For situations such as these, we have the","'99.47143555"
"'1","'LINE","'the character of the distribution FT. As noted above, this","'99.07563782"
"'1","'LINE","'following result.","'99.76306915"
"'1","'LINE","'distribution is strongly influenced by unequal sample sizes","'99.84589386"
"'1","'LINE","'Theorem. For a fixed observation Z, the classifier g given in (6)","'97.90313721"
"'1","'LINE","'(no # n1), unequal subset sizes (ro # r1), and/or unequal","'93.72792816"
"'1","'LINE","'based on the statistic T given in (4) is consistent against","'99.62080383"
"'1","'LINE","'rank choices (ko # k1). In effect, (6) implicitly weights the","'95.57059479"
"'1","'LINE","'alternatives of stochastic ordering of the class-conditional","'99.54277039"
"'1","'LINE","'ensemble votes in a probabilistically appropriate way.","'99.07470703"
"'1","'LINE","'interpoint distance distributions.","'99.09804535"
"'1","'LINE","'The combination methodology employed here is","'99.87330627"
"'1","'LINE","'straightforward and allows for analysis via mathematical","'99.83787537"
"'1","'LINE","'Proof. For fixed values of ro,r1, ko, k1, as No,N1 -> 00 with","'85.92448425"
"'1","'LINE","'statistics. However, more elaborate combination methods","'99.65648651"
"'1","'LINE","'(0,1), T(2) is asymptotically normal","'92.07723999"
"'1","'LINE","'such as those presented in [20] may be beneficial in terms of","'99.89689636"
"'1","'LINE","'under and lim -1-1/2) = T a.s., where T is given by","'90.56626892"
"'1","'LINE","'classification performance. In particular, using fewer care-","'99.54562378"
"'1","'LINE","'(8). Under HA p(z, p(z,Xi|Y = 1) (see, for","'86.10098267"
"'1","'LINE","'fully selected subsets SO that the ensemble does not employ","'98.03803253"
"'1","'LINE","'example, Fig. 3, left panel) lim T(2) > a.s. and","'97.19364166"
"'1","'LINE","'SO many classifiers is worthy of consideration, but makes","'97.87752533"
"'1","'LINE","'lim I{7(z)>F;'(1/2)} a.s., while under HA p(z,Xi|Yi =","'78.40142059"
"'1","'LINE","'the analysis of the statistic significantly more difficult.","'99.59114075"
"'1","'LINE","'0) p(z,X|Y;=1) (see, for example, Fig. 3, right panel)","'88.58749390"
